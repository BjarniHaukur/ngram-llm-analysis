wandb_project: ngram-llm-analysis

tokenizer_name: tokenizer_8192  # should match vocab_size
vocab_size: 8192 # 65536 for u16 ngram trie

model_type: llama
hidden_size: 2048
num_hidden_layers: 24
num_attention_heads: 16
intermediate_size: 8192
max_position_embeddings: 2048
