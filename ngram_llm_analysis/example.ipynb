{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5h/bc4n_w3d6pbcdc772w_c6tsc0000gn/T/ipykernel_17205/3859147415.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"../46012.pt\", map_location=\"cpu\")[\"model_state_dict\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from utils.tokenizer import load_tokenizer\n",
    "from train import model_from_config\n",
    "\n",
    "tok = load_tokenizer(\"tokenizer\")\n",
    "\n",
    "with open(\"../data/small_train.txt\", \"r\") as f:\n",
    "    f.readline() # the first line is just \"text\"\n",
    "    data = f.read()\n",
    "    \n",
    "with open(\"../configs/llama_medium.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "model = model_from_config(config)\n",
    "state_dict = torch.load(\"../46012.pt\", map_location=\"cpu\")[\"model_state_dict\"]\n",
    "state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}  # Remove \"_orig_mod.\" prefix from keys\n",
    "model.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, Felix was feeling very sleepy . He wanted to take a nap , but he was so sleepy that he couldn ' t . He decided to take a nap in the morning . When he woke up , he felt much better . He was so happy that he decided to take a nap right there in the morning . He was so happy that he had taken a nap in the morning . The end .\" \" Once upon a time , there was a girl named Sarah . She was three years old and loved to play . One day , Sarah was playing in the park when she saw a big puddle . She was so excited and wanted to jump in it . She "
     ]
    }
   ],
   "source": [
    "from utils.sample import sample_with_temp, nucleus_sample\n",
    "import torch\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=0.7):\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode the prompt to input IDs\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt, add_special_tokens=False).ids).unsqueeze(0)\n",
    "    \n",
    "    generated = []\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            # next_token = nucleus_sample(next_token_logits, 0.4)\n",
    "            next_token = sample_with_temp(next_token_logits, temperature)\n",
    "        \n",
    "        generated.append(next_token.item())\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)\n",
    "        \n",
    "        # Check for end-of-sequence token\n",
    "        if next_token.item() == tokenizer.token_to_id(\"<eos>\"):\n",
    "            break\n",
    "    \n",
    "\n",
    "        token = tokenizer.id_to_token(next_token.item())\n",
    "\n",
    "        print(token, end=\" \")\n",
    "        \n",
    "\n",
    "# Example usage\n",
    "prompt = \"Once upon a time, Felix\"\n",
    "print(prompt, end=\" \")\n",
    "generate_text(model, tok, prompt, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/small_train.txt\", \"r\") as f:\n",
    "    f.readline() # the first line is just \"text\"\n",
    "    data = f.read()\n",
    "    \n",
    "# first 10000 words\n",
    "data = \" \".join(data.split()[:10000])\n",
    "    \n",
    "data = data.lower()\n",
    "data = data.replace(\".\", \"\")\n",
    "data = data.replace(\",\", \"\")\n",
    "data = data.replace(\"!\", \"\")\n",
    "data = data.replace(\"?\", \"\")\n",
    "data = data.replace(\"'\", \"\")\n",
    "data = data.replace(\"\\\"\", \"\")\n",
    "\n",
    "word_to_ids = {word: i for i, word in enumerate(set(data.split()))}\n",
    "ids_to_word = {i: word for word, i in word_to_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = [word_to_ids[word] for word in data.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import NGramTrie\n",
    "\n",
    "trie = NGramTrie.fit(tokenized_data, ngram_max_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 56)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trie.search([word_to_ids[word] for word in \"one day\".split()]), data.count(\"one day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 11)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trie.search([word_to_ids[word] for word in \"one day a\".split()]), data.count(\"one day a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 11)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trie.search([word_to_ids[word] for word in \"one day a\".split()], \"++*\"), data.count(\"one day a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 11),\n",
       " ('beep', 1),\n",
       " ('the', 7),\n",
       " ('while', 1),\n",
       " ('tim', 6),\n",
       " ('max', 1),\n",
       " ('when', 2),\n",
       " ('timmy', 1),\n",
       " ('his', 2),\n",
       " ('sue', 1),\n",
       " ('she', 8),\n",
       " ('he', 4),\n",
       " ('it', 1),\n",
       " ('zigzag', 1),\n",
       " ('they', 1),\n",
       " ('her', 1),\n",
       " ('tims', 1),\n",
       " ('joe', 1),\n",
       " ('mia', 1),\n",
       " ('lucy', 1),\n",
       " ('charlies', 1),\n",
       " ('sam', 1),\n",
       " ('bill', 1)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = trie.find_all_nodes([word_to_ids[word] for word in \"one day\".split()])\n",
    "[(ids_to_word[token], node.count) for token, node in node[0].children.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, 8)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count(\"one day while\"), data.count(\"one day tim\"), data.count(\"one day she\")  # why is tim wrong?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
